# ğŸ”Œ GuÃ­a de IntegraciÃ³n - Nodo de Profundidad con tu Pipeline RGB

## Resumen

El nodo `depth_inference_node.py` se conecta al tÃ³pico `/rgb_image` que ya estÃ¡ publicando en tu sistema.

**Objetivo**: Procesar cada imagen RGB y estimar la profundidad monocular en tiempo real usando MiDaS v2.1 Small (ONNX).

---

## Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Tu Nodo RGB           â”‚
â”‚ (Publica /rgb_image)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ sensor_msgs/Image (BGR8)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  depth_inference_node.py            â”‚
â”‚  (Se suscribe a /rgb_image)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Recibe imagen RGB                â”‚
â”‚ 2. Preprocesa (resize, normalize)   â”‚
â”‚ 3. Inferencia ONNX MiDaS v2.1      â”‚
â”‚ 4. Postprocesa (normaliza, resize)  â”‚
â”‚ 5. Publica resultados               â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â–º /camera/depth_estimated (mono16)
       â”‚   Valores: 0-65535 (normalizado 0-1)
       â”‚
       â””â”€â–º /camera/depth_colored (bgr8)
           VisualizaciÃ³n con colormap TURBO
```

---

## ğŸ“‹ Requisitos Previos

### 1. Modelo ONNX Descargado

```bash
cd ~/ros2_ws/src/nomeer_robot_ros2/src/mono_depth_onnx

# Verificar que existe:
ls -lh models/midas_v21_small.onnx

# Si no existe, descargar:
python3 scripts/download_midas_model.py
```

**Output esperado**: Archivo `models/midas_v21_small.onnx` (~160 MB)

### 2. Dependencias ROS 2

```bash
pip3 install onnxruntime onnxscript torch
```

### 3. Paquete Compilado

```bash
cd ~/ros2_ws
colcon build --packages-select mono_depth_onnx
source install/setup.bash
```

---

## ğŸš€ EjecuciÃ³n Paso a Paso

### Paso 1: Verificar Tu Nodo de ImÃ¡genes

En **Terminal 1**, inicia tu nodo que publica RGB:

```bash
# Ejemplo genÃ©rico:
source ~/ros2_ws/install/setup.bash
ros2 run [tu_paquete] [tu_nodo]
```

Verifica que publica:
```bash
# En otra terminal:
ros2 topic list | grep rgb_image
ros2 topic echo /rgb_image --once
```

**Salida esperada**: Mensaje `sensor_msgs/Image` con el frame actual.

---

### Paso 2: Lanzar Nodo de Profundidad

En **Terminal 2**:

```bash
source ~/ros2_ws/install/setup.bash
ros2 run mono_depth_onnx depth_inference_node.py
```

**Salida esperada** (logs):

```
[depth_inference] INFO: Depth Inference Node initialized
[depth_inference] INFO:   Model: midas_v21_small
[depth_inference] INFO:   Model path: models/midas_v21_small.onnx
[depth_inference] INFO:   Input size: 256x256
[depth_inference] INFO:   GPU enabled: False
[depth_inference] INFO: Loading ONNX model with providers: ['CPUExecutionProvider']
[depth_inference] INFO: Model loaded successfully
[depth_inference] INFO:   Input name: input_rgb, shape: ['1', '3', '256', '256']
[depth_inference] INFO:   Output name: output, shape: ['1', '1', '256', '256']
```

Si ves errores:
- **"Model not found"** â†’ Ejecutar `scripts/download_midas_model.py`
- **"onnxruntime not installed"** â†’ `pip3 install onnxruntime`
- **"Failed to initialize depth model"** â†’ Ver logs en `install/setup.bash`

---

### Paso 3: Monitorear EjecuciÃ³n

En **Terminal 3**:

```bash
# Ver FPS en tiempo real
watch -n 1 'ros2 topic hz /camera/depth_estimated'

# O ver la frecuencia:
ros2 topic hz /camera/depth_colored
```

**Salida esperada**:

```
average rate: 10.5 Hz
  min: 0.084s max: 0.124s std dev: 0.019s count: 10
```

---

## ğŸ“Š Verificar Datos Publicados

### Topic 1: Depth Estimado (16-bit)

```bash
ros2 topic echo /camera/depth_estimated --once
---
header:
  seq: 42
  stamp:
    sec: 1707861234
    nsec: 567891234
  frame_id: camera
height: 480
width: 640
encoding: mono16
is_bigendian: false
step: 1280
data: [256, 128, 512, ...]
```

- **Encoding**: `mono16` (16-bit unsigned)
- **Rango**: 0-65535 (donde 0=profundo, 65535=cerca)
- **ResoluciÃ³n**: Mismo tamaÃ±o que imagen entrada

### Topic 2: Depth Visualizado (BGR8)

```bash
# Para visualizar con rviz2:
rviz2

# En rviz2:
# 1. Add â†’ Image
# 2. Topic: /camera/depth_colored
# 3. Encoder: default
```

- **Encoding**: `bgr8` (color)
- **Colormap**: TURBO (rojo=profundo, azul=cerca)

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### ParÃ¡metros del Nodo

Pasarlos en lÃ­nea de comandos:

```bash
# Cambiar modelo dinamicamente
ros2 run mono_depth_onnx depth_inference_node.py \
  --ros-args \
  -p model_path:=models/midas_v21_small.onnx \
  -p enable_gpu:=true
```

**ParÃ¡metros disponibles**:
- `model_path`: Ruta del archivo ONNX
- `model_name`: Nombre del modelo (para logs)
- `input_height`: Altura preprocesamiento (default: 256)
- `input_width`: Ancho preprocesamiento (default: 256)
- `enable_gpu`: Usar GPU si estÃ¡ disponible (default: false)

### Verificar Providers Disponibles

```bash
python3 << 'EOF'
import onnxruntime as ort
print("Providers disponibles:", ort.get_available_providers())
EOF
```

**Salida esperada**:

```
Providers disponibles: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
```

---

## ğŸ” Debugging ComÃºn

### Problema: "TÃ³pico /rgb_image no encontrado"

```bash
# Verificar que tu nodo estÃ¡ corriendo
ros2 node list | grep [tu_nodo]

# Ver todos los tÃ³picos disponibles
ros2 topic list

# Subscribirse manualmente a /rgb_image
ros2 topic echo /rgb_image
```

**SoluciÃ³n**: Iniciar tu nodo de imÃ¡genes antes que `depth_inference_node.py`.

---

### Problema: "FPS muy bajo (< 1 Hz)"

**Causas posibles**:
1. CPU sobrecargada
2. ImÃ¡genes muy grandes (resize lento)
3. ONNX Runtime no optimizado

**Soluciones**:
```bash
# Reducir tamaÃ±o de entrada (mÃ¡s rÃ¡pido, menos preciso)
ros2 run mono_depth_onnx depth_inference_node.py \
  --ros-args \
  -p input_height:=128 \
  -p input_width:=128

# O habilitar GPU (si estÃ¡ disponible)
ros2 run mono_depth_onnx depth_inference_node.py \
  --ros-args \
  -p enable_gpu:=true
```

---

### Problema: "Error: CUDA not found"

```bash
# Esto es NORMAL. El nodo fallback automÃ¡ticamente a CPU.
# SoluciÃ³n: Usar CPU (funciona perfecto, solo mÃ¡s lento)
ros2 run mono_depth_onnx depth_inference_node.py \
  --ros-args \
  -p enable_gpu:=false
```

---

## ğŸ“ˆ Rendimiento Esperado

| Hardware | FPS (256x256) | Tiempo/Frame |
|----------|---------------|-------------|
| CPU (i7) | 5-10          | 100-200 ms  |
| CPU (i5) | 3-5           | 200-300 ms  |
| GPU (RTX) | 30-60         | 16-33 ms    |

---

## ğŸ¯ PrÃ³ximos Pasos: Usar el Depth en tu AplicaciÃ³n

Una vez que tienes `/camera/depth_estimated` y `/camera/depth_colored` publicados:

### OpciÃ³n 1: Visualizar en RViz2

```bash
rviz2

# AÃ±adir:
1. Image â†’ Topic: /camera/depth_colored
2. Image â†’ Topic: [tu_rgb_image]
```

### OpciÃ³n 2: Procesar en C++

```cpp
#include <rclcpp/rclcpp.hpp>
#include <sensor_msgs/msg/image.hpp>

void depth_callback(const sensor_msgs::msg::Image::SharedPtr msg) {
    // Procesar depth map...
}

rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr sub = 
    node->create_subscription<sensor_msgs::msg::Image>(
        "/camera/depth_estimated", 10, depth_callback);
```

### OpciÃ³n 3: Procesar en Python

```python
def depth_callback(msg):
    from cv_bridge import CvBridge
    import numpy as np
    
    bridge = CvBridge()
    depth_cv = bridge.imgmsg_to_cv2(msg, desired_encoding='mono16')
    depth_float = depth_cv.astype(np.float32) / 65535.0
    
    # Usar depth_float para tus algoritmos...
```

---

## ğŸ“ Checklist Final

```
[ ] Modelo midas_v21_small.onnx existe en models/
[ ] Tu nodo publica en /rgb_image
[ ] depth_inference_node.py inicia sin errores
[ ] /camera/depth_estimated se publica
[ ] /camera/depth_colored se publica
[ ] FPS es razonable (> 1 Hz)
[ ] Depth values en rango 0-65535
[ ] Puedo visualizar en RViz2
```

---

## ğŸ†˜ Soporte

Si hay problemas:

1. **Revisar logs**: `ros2 run mono_depth_onnx depth_inference_node.py 2>&1 | tail -50`
2. **Verificar modelo**: `ls -lh models/midas_v21_small.onnx`
3. **Test inferencia**: `python3 -c "import onnxruntime; print(onnxruntime.__version__)"`
4. **Ver README.md**: DocumentaciÃ³n tÃ©cnica completa

---

**Â¡Listo!** ğŸ‰ Tu pipeline de profundidad estÃ¡ integrado. ContinÃºa con tu aplicaciÃ³n.
