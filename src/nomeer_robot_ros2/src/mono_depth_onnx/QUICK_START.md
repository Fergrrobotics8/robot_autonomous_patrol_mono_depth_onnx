# QUICK START - Mono Depth ONNX

## ðŸš€ 5 minutos para empezar

### Paso 1: Compilar (1 minuto)

```bash
cd ~/ros2_ws
colcon build --packages-select mono_depth_onnx
source install/setup.bash
```

**Status**: âœ… Si compila sin errores

### Paso 2: Instalar Dependencias

```bash
pip3 install torch timm opencv-python onnx onnxruntime onnxscript

# IMPORTANTE: Downgrade NumPy (compatibilidad con cv_bridge)
pip install 'numpy<2'
```

**Â¿Por quÃ© numpy<2?** ROS 2 Humble usa cv_bridge compilado con NumPy 1.x


**Descargar y convertir el modelo MiDaS v2.1 Small (todo automÃ¡tico):**
```bash
cd ~/ros2_ws/src/nomeer_robot_ros2/src/mono_depth_onnx
python3 scripts/download_midas_model.py
```

**Output**: `models/midas_v21_small.onnx` creado

### Paso 3: Preparar ImÃ¡genes (30 segundos)

```bash
# Si no tienes imÃ¡genes, crear carpeta
mkdir -p data/images

# OpciÃ³n A: Usar imÃ¡genes de prueba (si tienes)
cp /ruta/a/tus/imÃ¡genes/*.jpg data/images/

# OpciÃ³n B: Si no tienes imÃ¡genes
# Ver secciÃ³n "Obtener Datos de Prueba" abajo
```

### Paso 4: Ejecutar Nodo de Inferencia (30 segundos)

El nodo de inferencia **se conecta automÃ¡ticamente** al tÃ³pico `/rgb_image` que publicas.

En terminal 1 (inicia tu nodo que publica RGB):
```bash
ros2 run [tu_paquete] [tu_nodo]
```

En terminal 2 (inicia el nodo de profundidad):
```bash
source ~/ros2_ws/install/setup.bash
ros2 run mono_depth_onnx depth_inference_node.py
```

**Esperado**: 
- VerÃ¡s logs mostrando FPS, tiempo de inferencia (~100-300ms por frame en CPU)
- Topics publicados: `/camera/depth_estimated` (16-bit depth) y `/camera/depth_colored` (visualizaciÃ³n)

### Paso 5: Verificar Salida (en otra terminal)

```bash
# Ver depth map (16-bit)
ros2 topic echo /camera/depth_estimated --once

# Ver depth visualizado (con colormap)
ros2 topic echo /camera/depth_colored --once

# Ver topics disponibles
ros2 topic list | grep depth
```

---

## ðŸŽ¯ VerificaciÃ³n RÃ¡pida

### Checklist:

```
[ ] Paquete compilÃ³ sin errores
[ ] Modelo descargado (models/midas_v21_small.onnx existe)
[ ] Nodo de imÃ¡genes RGB en ejecuciÃ³n (publicando en /rgb_image)
[ ] Nodo `depth_inference_node.py` ejecutÃ¡ndose sin errores
[ ] Topics publicando datos (ros2 topic list | grep depth)
```

---

## ðŸ”Œ IntegraciÃ³n Detallada

### Flujo de Datos:

1. **Tu nodo** â†’ publica en `/rgb_image` (sensor_msgs/Image)
2. **depth_inference_node.py** â†’ se suscribe a `/rgb_image`
3. **Procesa** â†’ ejecuta inferencia con MiDaS v2.1 ONNX
4. **Publica**:
   - `/camera/depth_estimated`: 16-bit depth map (mono16)
   - `/camera/depth_colored`: Depth visualizado con colormap (bgr8)

### ConfiguraciÃ³n del Nodo:

```bash
# ParÃ¡metros por defecto (en el cÃ³digo):
- model_path: models/midas_v21_small.onnx
- model_name: midas_v21_small
- input_height: 256
- input_width: 256
- enable_gpu: false
- subscription_topic: /rgb_image
```

### Prueba Manual:

```bash
# Terminal 1: Tu nodo de imÃ¡genes
ros2 run [tu_paquete] [tu_nodo]

# Terminal 2: Nodo de profundidad
source ~/ros2_ws/install/setup.bash
ros2 run mono_depth_onnx depth_inference_node.py

# Terminal 3: Monitoreo
ros2 topic hz /camera/depth_estimated
ros2 topic echo /camera/depth_estimated --once
```

### Cambiar ParÃ¡metros (lÃ­nea de comandos):

```bash
ros2 run mono_depth_onnx depth_inference_node.py \
  --ros-args \
  -p model_path:=models/midas_v21_small.onnx \
  -p enable_gpu:=false
```



### OpciÃ³n 1: Descargar Dataset Online

```bash
# Crear directorio
mkdir -p mono_depth_onnx/data/images
cd mono_depth_onnx/data/images

# Descargar algunas imÃ¡genes de ejemplo (CC0 license)
# Ejemplo: COCO dataset, unsplash, etc.

# O simplemente copiar cualquier imagen:
cp ~/Pictures/*.jpg .
```

### OpciÃ³n 2: Generar ImÃ¡genes desde Video

```bash
# Si tienes un video:
ffmpeg -i video.mp4 -vf fps=2 data/images/frame_%04d.jpg
```

### OpciÃ³n 3: Usar Webcam Directamente

Cambiar en `config/mono_depth_config.yaml`:

```yaml
image_source:
  source_type: "webcam"
  source_path: "0"
```

Luego ejecutar: `ros2 launch mono_depth_onnx full_pipeline.launch.py`

---

## ðŸŽ¯ VerificaciÃ³n RÃ¡pida

### Checklist:

```
[ ] Paquete compilÃ³ sin errores
[ ] Modelo descargado (models/midas_v21_small.onnx existe)
[ ] ImÃ¡genes en data/images/ (3+ imÃ¡genes)
[ ] Launch file ejecutado sin crashes
[ ] Topics publicando datos (ros2 topic list)
[ ] MÃ©tricas en results/depth_metrics.json despuÃ©s de unos segundos
```

---

## ðŸ”§ ConfiguraciÃ³n ComÃºn

depth_estimator:
### Para Desarrollo RÃ¡pido
En `config/mono_depth_config.yaml`:

```yaml
image_source:
  publish_frequency: 30.0      # MÃ¡s rÃ¡pido
depth_estimator:
  model_name: "midas_v21_small" # PequeÃ±o, rÃ¡pido y recomendado
  enable_gpu: false            # CPUs ok
```

depth_estimator:
depth_metric:
### Para PrecisiÃ³n
```yaml
depth_estimator:
  model_name: "midas_v21_small" # (o el modelo ONNX que tengas)
  enable_gpu: true             # GPU recomendado si estÃ¡ disponible
depth_metric:
  outlier_filter_type: "iqr"   # Mejor filtrado
```

depth_estimator:
### Para GPU
```yaml
depth_estimator:
  enable_gpu: true             # Activar CUDA
```

Verificar: `python3 -c "import onnxruntime; print(onnxruntime.get_available_providers())"`

---

## ðŸ› Problemas Comunes

| Problem | SoluciÃ³n |
|---------|----------|
| "Model not found" | Descargar manualmente el modelo y colocarlo en `models/` |
| "No images found" | `mkdir -p data/images && cp *.jpg data/images/` |
| "ONNX Runtime not found" | `pip3 install onnxruntime` |
| "Low FPS" | Usar `midas_v3_small`, reduce `publish_frequency` |
| "Import error cv_bridge" | `pip3 install cv-bridge` |
| "CUDA not found" | OK, usar CPU (mÃ¡s lento pero funciona) |

---

## ðŸ“Š Monitoreo

### Ver Performance

```bash
# FPS de inferencia
watch -n 1 'ros2 topic hz /camera/depth_estimated'

# FPS de mÃ©tricas
watch -n 1 'ros2 topic hz /depth_metric/min_frontal_depth'

# CPU/Memory (en otra terminal)
top
```

### Ver Datos

```bash
# Profundidad mÃ­nima
ros2 topic echo /depth_metric/min_frontal_depth --once

# Ver todos los tÃ³picos
ros2 topic list | grep camera
ros2 topic list | grep depth
```

---

## ðŸ’¾ Resultados Generados

DespuÃ©s de ejecutar, verÃ¡s:

- `results/depth_metrics.json`: MÃ©tricas guardadas
- `results/safety_events.json`: Eventos (si safety activo)

Ver contenido:
```bash
cat mono_depth_onnx/results/depth_metrics.json | python3 -m json.tool
```

---

## ðŸ”— IntegraciÃ³n con AutonomÃ­a (Opcional)

Ejecutar todos los componentes:

**Terminal 1**: AutonomÃ­a
```bash
ros2 launch autonomous_patrol follow_waypoints.launch.py
```

**Terminal 2**: Profundidad
```bash
ros2 launch mono_depth_onnx with_autonomy.launch.py
```

Resultado: Robot se detiene si detecta obstÃ¡culo delante (seguridad).

---

## ðŸ“š MÃ¡s InformaciÃ³n

- **README.md**: DocumentaciÃ³n tÃ©cnica completa
- **RESUMEN_ES.md**: Resumen en espaÃ±ol
- **scripts/download_midas_model.py**: Script de modelo
- **config/mono_depth_config.yaml**: Todas las opciones

---

## ðŸš¨ Testing Checklist

```bash
# Test 1: CompilaciÃ³n
colcon build --packages-select mono_depth_onnx

# Test 2: Modelo
python3 scripts/download_midas_model.py --model midas_v21_small

# Test 3: Nodo inferencia
ros2 run mono_depth_onnx depth_inference_node.py

# Test 4: Nodo mÃ©tricas
ros2 run mono_depth_onnx depth_metric_node.py

# Test 5: Pipeline completo
ros2 launch mono_depth_onnx full_pipeline.launch.py

# Test 6: Con autonomÃ­a
# [Iniciar autonomÃ­a primero]
ros2 launch mono_depth_onnx with_autonomy.launch.py
```

---

**Â¡Listo!** ðŸŽ‰ Ya tienes el sistema de profundidad con IA corriendo.

Para detalles avanzados, ver README.md completo.
